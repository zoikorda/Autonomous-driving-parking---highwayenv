{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Αντίγραφο CEMplanner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoikorda/self-driving-cars/blob/main/model-based_CEMplanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMSuJEOfviP",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "# Install environment and visualization dependencies \n",
        "!pip install highway-env\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -y\n",
        "\n",
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Models \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimizer\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video(path):\n",
        "    html = []\n",
        "    for mp4 in Path(path).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvUYSL7sfvie",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "Tuple = namedtuple('Tuple', ['state', 'action', 'next_state'])\n",
        "env = gym.make(\"parking-v0\")\n",
        "\n",
        "# Collect random experiences from the environment\n",
        "def data_collection(env, size=1000, action_repeat=2): # size refers to the number of samples to be collected\n",
        "    data, done = [], True\n",
        "    for _ in trange(size, desc=\"Random collection of data from interaction with environment\"):\n",
        "        action = env.action_space.sample()\n",
        "        #print(env.action_space) # the action is a one dimension matrix with to features normalised from -1 to +1 a\n",
        "        #print(env.action_space.sample())\n",
        "        for _ in range(action_repeat):\n",
        "            previous_observation = env.reset() if done else observation\n",
        "            #print(action)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            data.append(Tuple(torch.Tensor(previous_observation[\"observation\"]),\n",
        "                                   torch.Tensor(action),\n",
        "                                   torch.Tensor(observation[\"observation\"])))\n",
        "    return data\n",
        "\n",
        "data = data_collection(env)\n",
        "print(\"Sample transition:\", data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Gl2kKJfviu",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "class DynamicsModel(nn.Module):\n",
        "    STATE_X = 0\n",
        "    STATE_Y = 1\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidden_size, dt):\n",
        "        super().__init__()\n",
        "        self.state_size, self.action_size, self.dt = state_size, action_size, dt\n",
        "        A_size = state_size * state_size # 6*6=36\n",
        "        #print(A_size)\n",
        "        B_size = state_size * action_size # 6*2=12\n",
        "        #print(B_size)\n",
        "        self.A_enter = nn.Linear(state_size + action_size, hidden_size) # Applies a linear transformation to the incoming data: y = xA^T + b\n",
        "        #print(self.A_enter)\n",
        "        self.A_exit = nn.Linear(hidden_size, A_size)\n",
        "        #print(self.A_exit)\n",
        "        self.B_enter = nn.Linear(state_size + action_size, hidden_size)\n",
        "        #print(self.B_enter)\n",
        "        self.B_exit = nn.Linear(hidden_size, B_size)\n",
        "        #print(self.B_exit)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        \"\"\"\n",
        "            Predict x_{t+1} = f(x_t, a_t)\n",
        "        :param x: a batch of states\n",
        "        :param a: a batch of actions\n",
        "        \"\"\"\n",
        "        xa = torch.cat((x, a), -1) # Concatenates the given sequence of seq tensors in the given dimension.\n",
        "        \n",
        "        xa[:, self.STATE_X:self.STATE_Y+1] = 0  # Remove dependency in (x,y)\n",
        "        #print(self.A_enter(xa))\n",
        "        #print(F.relu(self.A_enter(xa))) # ReLu returns 0 if the input is negative otherwise return the input as it is.\n",
        "        A = self.A_exit(F.relu(self.A_enter(xa)))\n",
        "        #print(A.shape)\n",
        "        A = torch.reshape(A, (x.shape[0], self.state_size, self.state_size)) # reshape the [1,36] A matrix to [1,6,6] matrix\n",
        "        #print(x.shape) # [1,6]\n",
        "        #print(A.shape) # [1,6,6]\n",
        "        #print(A)\n",
        "        B = self.B_exit(F.relu(self.B_enter(xa)))\n",
        "        B = torch.reshape(B, (x.shape[0], self.state_size, self.action_size))\n",
        "        dx = A @ x.unsqueeze(-1) + B @ a.unsqueeze(-1)\n",
        "        #print(self.dt) # self.dt = 0.2 \n",
        "        return x + dx.squeeze()*self.dt\n",
        "\n",
        "\n",
        "dynamics = DynamicsModel(state_size=env.observation_space.spaces[\"observation\"].shape[0],\n",
        "                         action_size=env.action_space.shape[0],\n",
        "                         hidden_size=64,\n",
        "                         dt=1/env.unwrapped.config[\"policy_frequency\"])\n",
        "print(\"Forward initial model on a sample transition:\", dynamics(data[0].state.unsqueeze(0),\n",
        "                                                                data[0].action.unsqueeze(0)).detach())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwCDLD1wfvi2",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "import torch.optim as optimizer\n",
        "optimizer = optimizer.Adam(dynamics.parameters(), lr=0.01)\n",
        "\n",
        "# Split dataset into training and validation\n",
        "train_ratio = 0.7\n",
        "train_data = data[:int(train_ratio * len(data))]\n",
        "#print(train_data)\n",
        "validation_data = data[int(train_ratio * len(data)):]\n",
        "#print(validation_data)\n",
        "\n",
        "def loss_function(model, data, loss_func = torch.nn.MSELoss()):\n",
        "    states, actions, next_states = data\n",
        "    predictions = model(states, actions)\n",
        "    return loss_func(predictions, next_states)\n",
        "    \n",
        "def train_model(model, train_data, validation_data, epochs=1500):\n",
        "    train_data_t = Tuple(*map(torch.stack, zip(*train_data)))\n",
        "    #print(train_data_t) #concentrate together all states, all actions, all next_states\n",
        "    validation_data_t = Tuple(*map(torch.stack, zip(*validation_data)))\n",
        "    losses = np.full((epochs, 2), np.nan) # epochs=1500 , fill the matrix with nan\n",
        "    for epoch in trange(epochs, desc=\"Train dynamics\"):\n",
        "        # Compute loss gradient and step optimizer\n",
        "        train_loss = loss_function(model, train_data_t)\n",
        "        #print(train_loss)\n",
        "        validation_loss = loss_function(model, validation_data_t)\n",
        "        losses[epoch] = [train_loss.detach().numpy(), validation_loss.detach().numpy()]\n",
        "        print(losses[epoch])\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "    # Plot losses\n",
        "    plt.plot(losses)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend([\"training\", \"validation\"])\n",
        "    plt.show()\n",
        "\n",
        "train_model(dynamics, data, validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMPA55bCfvjB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def predict_trajectory(current_state, actions, model, action_repeat=1):\n",
        "    states_predicted = []\n",
        "    for action in actions:\n",
        "        for _ in range(action_repeat):\n",
        "            #print(current_state)\n",
        "            current_state = model(current_state, action)\n",
        "            #print(current_state)\n",
        "            states_predicted.append(current_state)\n",
        "    return torch.stack(states_predicted, dim=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRubRv9buNXj"
      },
      "source": [
        "def reward_model(states, desired_state, gamma=None):\n",
        "    \"\"\"\n",
        "        The reward is a weighted L1-norm between the state and a desired state\n",
        "    :param Tensor states: a batch of states. shape: [batch_size, state_size].\n",
        "    :param Tensor goal: a desired state. shape: [state_size].\n",
        "    :param float gamma: a discount factor\n",
        "    \"\"\"\n",
        "    #print(\"states_shape\", states.shape) # states is a torch.Size([1,6])\n",
        "    desired_state = desired_state.expand(states.shape)\n",
        "    reward_weigths = torch.Tensor([1., 0.3, 0., 0., 0.02, 0.02])\n",
        "    #reward_weigths = torch.Tensor(env.unwrapped.REWARD_WEIGHTS)\n",
        "    # REWARD_WEIGHTS : [1.   0.3  0.   0.   0.02 0.02]\n",
        "    #print(reward_weigths) # torch.Size([6])\n",
        "    rewards = -torch.pow(torch.norm((states-desired_state)*reward_weigths, p=1, dim=-1), 0.5)\n",
        "    #print(rewards)\n",
        "    if gamma:\n",
        "        time = torch.arange(rewards.shape[0], dtype=torch.float).unsqueeze(-1).expand(rewards.shape)\n",
        "        print(\"TIME\", time)\n",
        "        rewards *= torch.pow(gamma, time)\n",
        "    #print(rewards)\n",
        "    return rewards\n",
        "\n",
        "observation = env.reset()\n",
        "#print(observation[\"observation\"])\n",
        "rewards = reward_model(torch.Tensor(observation[\"observation\"]).unsqueeze(0), torch.Tensor(observation[\"desired_goal\"]))\n",
        "print(\"Reward of a sample transition:\", rewards)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzPKYg23fvjL",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "source": [
        "def cem_planner(current_state, desired_state, action_size, horizon=5, population=100, selection=10, iterations=10):\n",
        "    \"\"\"\n",
        "      iterations (int): maximum number of training iterations\n",
        "      population (int): size of population at each iteration\n",
        "    \"\"\"\n",
        "    current_state = current_state.expand(population, -1) # Returns a new view of the :attr:`self` tensor with singleton dimensions expanded to a larger size.Passing -1 as the size for a dimension means not changing the size of that dimension.\n",
        "    #print(current_state.shape) # torch.Size([100, 6])\n",
        "    action_mean = torch.zeros(horizon, 1, action_size) #μ=0\n",
        "    #print(action_mean) # torch.Size([5, 1, 2]) full of zeros\n",
        "    action_std = torch.ones(horizon, 1, action_size) #σ=1\n",
        "    #print(action_std) # torch.Size([5, 1, 2]) full of ones\n",
        "    for _ in range(iterations):\n",
        "        # 1. Draw sample sequences of actions from a normal distribution\n",
        "        #print(torch.randn(horizon, population, action_size).shape) # torch.Size([5, 100, 2])\n",
        "        actions = action_mean + action_std * torch.randn(horizon, population, action_size)\n",
        "        #print(actions) # torch.Size([5, 100, 2])\n",
        "        min=env.action_space.low.min() # min = -1\n",
        "        max=env.action_space.high.max() # max = +1\n",
        "        actions = torch.clamp(actions, min, max) # Clamp all elements in input into the range [ min, max ]. Let min_value and max_value be min and max, respectively\n",
        "        states = predict_trajectory(current_state, actions, dynamics, action_repeat=5)\n",
        "        #print(states) # torch.Size([25, 100, 6])\n",
        "        # 2. Fit the distribution to the top-k performing sequences\n",
        "        #print(desired_state.shape) # torch.Size([6])\n",
        "        rewards = reward_model(states, desired_state).sum(dim=0) # numpy is summing across the first (0th) and only axis\n",
        "        #print(rewards) # torch.Size([100]\n",
        "        elite_rewards, elite_rewards_indices = rewards.topk(selection, largest=True, sorted=False) # Returns the k(10) largest elements of the given input tensor along a given dimension\n",
        "        #print(elite_rewards) # torch.Size([10])\n",
        "        #print(elite_rewards_indices) # torch.Size([10])\n",
        "        elite_actions = actions[:, elite_rewards_indices, :] # concentrate the actions that give the maximum rewards\n",
        "        #print(elite_actions) # torch.Size([5, 10, 2])\n",
        "        action_mean, action_std = elite_actions.mean(dim=1, keepdim=True), elite_actions.std(dim=1, unbiased=False, keepdim=True)\n",
        "        #print(action_mean) # torch.Size([5, 1, 2])\n",
        "        #print(action_std) # torch.Size([5, 1, 2])\n",
        "        #print(action_mean[0]) # torch.Size([1, 2])\n",
        "        #print(action_mean[0].squeeze(dim=0)) # torch.Size([2])\n",
        "    return action_mean[0].squeeze(dim=0), elite_rewards\n",
        " \n",
        "  \n",
        "# Run the planner on a sample transition\n",
        "action, rewards = cem_planner(torch.Tensor(observation[\"observation\"]),\n",
        "                     torch.Tensor(observation[\"desired_goal\"]),\n",
        "                     env.action_space.shape[0])\n",
        "\n",
        "print(\"Planned action:\", action)\n",
        "rewards = rewards.detach().numpy() # these are the elite rewards of each episode\n",
        "print(rewards)\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(rewards)+1), rewards)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sxhf1Q6NlL"
      },
      "source": [
        "obs = env.reset()\n",
        "done =  False\n",
        "\n",
        "# Evaluate the agent\n",
        "rewards_total = []\n",
        "episode_reward = 0\n",
        "for _ in range(10000):\n",
        "  action, rewards = cem_planner(torch.Tensor(obs[\"observation\"]), torch.Tensor(obs[\"desired_goal\"]), env.action_space.shape[0])\n",
        "  obs, reward, done, info = env.step(action.numpy())\n",
        "  episode_reward += reward\n",
        "  if done or info.get('is_success', False):\n",
        "    print(\"Reward:\", episode_reward, \"Success?\", info.get('is_success', False))\n",
        "    rewards_total.append(episode_reward)\n",
        "    episode_reward = 0.0\n",
        "    obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq63cXwWQhf3"
      },
      "source": [
        "print(rewards_total)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(rewards_total)+1), rewards_total)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOcOP7Of18T2"
      },
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "for episode in trange(5, desc=\"Test episodes\"):\n",
        "    observation, done = env.reset(), False\n",
        "    while not done:\n",
        "        action, rewards = cem_planner(torch.Tensor(observation[\"observation\"]),\n",
        "                             torch.Tensor(observation[\"desired_goal\"]),\n",
        "                             env.action_space.shape[0])\n",
        "        observation, reward, done, info = env.step(action.numpy())\n",
        "    print(\"REWARDS\", rewards)\n",
        "env.close()\n",
        "show_video('./video')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}