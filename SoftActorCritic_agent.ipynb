{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Αντίγραφο SacActorCritic_parking.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmhw0/i+fi0UZjYKpvYW7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoikorda/self-driving-cars/blob/main/SoftActorCritic_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orjJUEeech1i"
      },
      "source": [
        "!pip install rltorch\n",
        "!pip install highway-env\n",
        "!pip install stable-baselines==2.10.0\n",
        "!pip install gym\n",
        "!pip install highway-env\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsl5UKv_AvxK"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBRHArWcRRpn"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimizer\n",
        "import numpy as np\n",
        "from torch.distributions import Normal\n",
        "from rltorch.network import create_linear_network\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from rltorch.memory import MultiStepMemory, PrioritizedMemory\n",
        "#import wandb\n",
        "from collections import deque\n",
        "from random import sample, random\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "from random import sample\n",
        "\n",
        "import gym\n",
        "import highway_env\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class Sarsd:\n",
        "  state: Any\n",
        "  action: int\n",
        "  reward: float\n",
        "  next_state: Any\n",
        "  masked_done: bool\n",
        "  episode_done: bool\n",
        "\n",
        "#state, action, reward, next_state, masked_done, episode_done=done))\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        #print(input_shape) # (6,)\n",
        "        self.memory_size = max_size  # the max size of the buffer/memory\n",
        "        self.memory_counter = 0  # counts the inputs\n",
        "        self.state_memory = np.zeros((self.memory_size, *input_shape))\n",
        "        #print(self.state_memory) # (2,6)\n",
        "        self.next_state_memory = np.zeros((self.memory_size, *input_shape))\n",
        "        #print(self.next_state_memory) # (2,6)\n",
        "        self.action_memory = np.zeros((self.memory_size, n_actions))\n",
        "        #print(self.action_memory) # (2,2)\n",
        "        self.reward_memory = np.zeros(self.memory_size)\n",
        "        self.done_memory = np.zeros(self.memory_size, dtype=np.bool)\n",
        "        \n",
        "    def insert(self, state, action, reward, next_state, episode_done):\n",
        "        index = self.memory_counter % self.memory_size # if the samples are more than the max size, the buffer is overwrited \n",
        "        #print(state) # dict of arrays, size 3\n",
        "\n",
        "        self.state_memory[index] = state[\"observation\"]\n",
        "        self.next_state_memory[index] = next_state[\"observation\"]\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.done_memory[index] = episode_done\n",
        "\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_memory = min(self.memory_counter, self.memory_size)\n",
        "        #print(max_memory)\n",
        "\n",
        "        batch = np.random.choice(max_memory, batch_size) # Generate a uniform random sample from np.arange(max_memory) of size batch_size\n",
        "        #print(batch)\n",
        "\n",
        "        states = torch.FloatTensor(self.state_memory[batch])\n",
        "        next_states = torch.FloatTensor(self.next_state_memory[batch])\n",
        "        actions = torch.FloatTensor(self.action_memory[batch])\n",
        "        rewards = torch.FloatTensor(self.reward_memory[batch])\n",
        "        dones = torch.FloatTensor(self.done_memory[batch])\n",
        "        \n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "#num_steps = 2\n",
        "env = gym.make(\"parking-v0\")\n",
        "\n",
        "#replay_buffer = ReplayBuffer(num_steps, env.observation_space[\"observation\"].shape, env.action_space.shape[0])\n",
        "\n",
        "\n",
        "class ReplayBuffer2:\n",
        "  def __init__(self, buffer_size=100000):\n",
        "    self.buffer_size = buffer_size\n",
        "    #self.buffer = []\n",
        "    self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "  def append(self, sars):\n",
        "    self.buffer.append(sars)\n",
        "    #self.buffer = self.buffer[-self.buffer_size:]\n",
        "\n",
        "  def sample(self, num_samples):\n",
        "    assert num_samples <= len(self.buffer)\n",
        "    return sample(self.buffer, num_samples)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "source": [
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video(path):\n",
        "    html = []\n",
        "    for mp4 in Path(path).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJenPpHARcxr"
      },
      "source": [
        "class BaseNetwork(nn.Module):\n",
        "    def save(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "class QNetwork(BaseNetwork):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_units=[256, 256],\n",
        "                 initializer='xavier'):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # https://github.com/ku2482/rltorch/blob/master/rltorch/network/builder.py\n",
        "        self.Q = create_linear_network(\n",
        "            num_inputs+num_actions, 1, hidden_units=hidden_units,\n",
        "            initializer=initializer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.Q(x)\n",
        "        return q\n",
        "\n",
        "\n",
        "class TwinnedQNetwork(BaseNetwork):\n",
        "\n",
        "    def __init__(self, num_inputs, num_actions, hidden_units=[256, 256],\n",
        "                 initializer='xavier'):\n",
        "        super(TwinnedQNetwork, self).__init__()\n",
        "\n",
        "        self.Q1 = QNetwork(\n",
        "            num_inputs, num_actions, hidden_units, initializer)\n",
        "        self.Q2 = QNetwork(\n",
        "            num_inputs, num_actions, hidden_units, initializer)\n",
        "\n",
        "    def forward(self, states, actions):\n",
        "        x = torch.cat([states, actions], dim=1)\n",
        "        q1 = self.Q1(x)\n",
        "        q2 = self.Q2(x)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class GaussianPolicy(BaseNetwork):\n",
        "    LOG_STD_MAX = 2\n",
        "    LOG_STD_MIN = -20\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, num_inputs, num_actions, hidden_units=[256, 256],\n",
        "                 initializer='xavier'):\n",
        "        super(GaussianPolicy, self).__init__()\n",
        "\n",
        "        # https://github.com/ku2482/rltorch/blob/master/rltorch/network/builder.py\n",
        "        self.policy = create_linear_network(\n",
        "            num_inputs, num_actions*2, hidden_units=hidden_units,\n",
        "            initializer=initializer)\n",
        "\n",
        "    def forward(self, states):\n",
        "        mean, log_std = torch.chunk(self.policy(states), 2, dim=-1)\n",
        "        log_std = torch.clamp(\n",
        "            log_std, min=self.LOG_STD_MIN, max=self.LOG_STD_MAX)\n",
        "        #print(mean, log_std)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, states):\n",
        "        # calculate Gaussian distribusion of (mean, std)\n",
        "        means, log_stds = self.forward(states)\n",
        "        stds = log_stds.exp()\n",
        "        normals = Normal(means, stds)\n",
        "        # sample actions\n",
        "        xs = normals.rsample()\n",
        "        actions = torch.tanh(xs)\n",
        "        # calculate entropies\n",
        "        log_probs = normals.log_prob(xs)\\\n",
        "            - torch.log(1 - actions.pow(2) + self.eps)\n",
        "        entropies = -log_probs.sum(dim=1, keepdim=True)\n",
        "\n",
        "        return actions, entropies, torch.tanh(means)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ2Lu2znRdyA"
      },
      "source": [
        "class RunningMeanStats:\n",
        "\n",
        "    def __init__(self, n=10):\n",
        "        self.n = n\n",
        "        self.stats = deque(maxlen=n)\n",
        "\n",
        "    def append(self, x):\n",
        "        self.stats.append(x)\n",
        "\n",
        "    def get(self):\n",
        "        return np.mean(self.stats)\n",
        "\n",
        "def to_batch(state, action, reward, next_state, done, device):\n",
        "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "    action = torch.FloatTensor([action]).view(1, -1).to(device)\n",
        "    reward = torch.FloatTensor([reward]).unsqueeze(0).to(device)\n",
        "    next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
        "    done = torch.FloatTensor([done]).unsqueeze(0).to(device)\n",
        "    return state, action, reward, next_state, done\n",
        "\n",
        "def hard_update(target, source):\n",
        "    target.load_state_dict(source.state_dict())\n",
        "\n",
        "def grad_false(network):\n",
        "    for param in network.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    for t, s in zip(target.parameters(), source.parameters()):\n",
        "        t.data.copy_(t.data * (1.0 - tau) + s.data * tau)\n",
        "\n",
        "def update_params(optimizer, network, loss, grad_clip=None, retain_graph=True):\n",
        "    optimizer.zero_grad()\n",
        "    #print(loss) # tensor(..., dtype=torch.float64)\n",
        "    torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xscfe3-4RhdY"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, observation_shape =  env.observation_space[\"observation\"].shape, num_steps=20000, batch_size=256,\n",
        "                 lr=0.0001, hidden_units=[256, 256], memory_size=1e6,\n",
        "                 gamma=0.99, tau=0.005, entropy_tuning=True, ent_coef=0.2,\n",
        "                 multi_step=1, per=False, alpha=0.6, beta=0.4,\n",
        "                 beta_annealing=0.0001, grad_clip=None, updates_per_step=1,\n",
        "                 start_steps=1000, log_interval=10, target_update_interval=1,\n",
        "                 eval_interval=1000, cuda=True, seed=0, verbose=1):\n",
        "        self.env = env\n",
        "        self.device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(num_steps, observation_shape, self.env.action_space.shape[0])\n",
        "        \n",
        "        self.num_steps = num_steps #20000\n",
        "        self.tau = tau\n",
        "        self.per = per\n",
        "        self.batch_size = batch_size\n",
        "        self.start_steps = start_steps #1000\n",
        "        self.gamma_n = gamma ** multi_step\n",
        "        self.entropy_tuning = entropy_tuning\n",
        "        self.grad_clip = grad_clip\n",
        "        self.updates_per_step = updates_per_step\n",
        "        self.log_interval = log_interval\n",
        "        self.target_update_interval = target_update_interval\n",
        "        self.eval_interval = eval_interval\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        self.env.seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True  # It harms a performance.\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        self.actor_model = GaussianPolicy(\n",
        "            self.env.observation_space[\"observation\"].shape[0],\n",
        "            self.env.action_space.shape[0],\n",
        "            hidden_units=hidden_units).to(self.device)\n",
        "        self.critic_model = TwinnedQNetwork(\n",
        "            self.env.observation_space[\"observation\"].shape[0],\n",
        "            self.env.action_space.shape[0],\n",
        "            hidden_units=hidden_units).to(self.device)\n",
        "        self.critic_target_model = TwinnedQNetwork(\n",
        "            self.env.observation_space[\"observation\"].shape[0],\n",
        "            self.env.action_space.shape[0],\n",
        "            hidden_units=hidden_units).to(self.device).eval()\n",
        "\n",
        "        # copy parameters of the learning network to the target network\n",
        "        hard_update(self.critic_target_model, self.critic_model)\n",
        "        # disable gradient calculations of the target network\n",
        "        grad_false(self.critic_target_model)\n",
        "\n",
        "        self.actor_optimizer = optimizer.Adam(self.actor_model.parameters(), lr=0.0001)\n",
        "        self.Q1_optimizer = optimizer.Adam(self.critic_model.Q1.parameters(), lr=0.0001)\n",
        "        self.Q2_optimizer = optimizer.Adam(self.critic_model.Q2.parameters(), lr=0.0001)\n",
        "\n",
        "        if entropy_tuning:\n",
        "            # Target entropy is -|A|.\n",
        "            self.target_entropy = -torch.prod(torch.Tensor(\n",
        "                self.env.action_space.shape).to(self.device)).item()\n",
        "            # We optimize log(alpha), instead of alpha.\n",
        "            self.log_alpha = torch.zeros(\n",
        "                1, requires_grad=True, device=self.device)\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "            self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
        "        else:\n",
        "            # fixed alpha\n",
        "            self.alpha = torch.tensor(ent_coef).to(self.device)\n",
        "\n",
        "        self.train_rewards = RunningMeanStats(log_interval)\n",
        "\n",
        "        self.steps = 0\n",
        "        self.learning_steps = 0\n",
        "        self.episodes = 0\n",
        "\n",
        "    def run(self, fuck):\n",
        "        while fuck!=1000:\n",
        "            fuck += 1\n",
        "            self.train_episode()\n",
        "            if self.steps > self.num_steps:\n",
        "                break\n",
        "\n",
        "    def is_update(self):\n",
        "        return self.replay_buffer.memory_size > self.batch_size and self.steps >= self.start_steps\n",
        "\n",
        "    def act(self, state):\n",
        "        if self.start_steps > self.steps:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.explore(state)\n",
        "        return action\n",
        "\n",
        "    def explore(self, state):\n",
        "        # act with randomness\n",
        "        state = torch.FloatTensor(state[\"observation\"]).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action, _, _ = self.actor_model.sample(state)\n",
        "        return action.cpu().numpy().reshape(-1)\n",
        "\n",
        "    def exploit(self, state):\n",
        "        # act without randomness\n",
        "        state = torch.FloatTensor(state[\"observation\"]).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            _, _, action = self.actor_model.sample(state)\n",
        "        return action.cpu().numpy().reshape(-1)\n",
        "\n",
        "    def calc_current_q(self, states, actions, rewards, next_states, dones):\n",
        "        curr_q1, curr_q2 = self.critic_model(states, actions)\n",
        "        #print(curr_q1.requires_grad) # True\n",
        "        return curr_q1, curr_q2\n",
        "\n",
        "    def calc_target_q(self, states, actions, rewards, next_states, dones):\n",
        "        with torch.no_grad():\n",
        "          next_actions, next_entropies, _ = self.actor_model.sample(next_states)\n",
        "          next_q1, next_q2 = self.critic_target_model(next_states, next_actions)\n",
        "          next_q = torch.min(next_q1, next_q2) + self.alpha * next_entropies\n",
        "          #print(next_q) # torch.Size([256, 1])\n",
        "          #print(dones) # (256,)\n",
        "          #print(rewards) # (256,)\n",
        "                    \n",
        "        target_q = rewards + (1.0 - dones) @ (self.gamma_n * next_q)\n",
        "        return target_q\n",
        "\n",
        "    def calculate_critic_loss(self, batch, weights):\n",
        "        #print(batch)\n",
        "        current_q1, current_q2 = self.calc_current_q(*batch)\n",
        "        target_q = self.calc_target_q(*batch)\n",
        "        #print(target_q.requires_grad) #True\n",
        "\n",
        "        # TD errors for updating priority weights\n",
        "        errors = torch.abs(current_q1.detach() - target_q)\n",
        "        # We log means of Q to monitor training.\n",
        "        mean_q1 = current_q1.detach().mean().item()\n",
        "        mean_q2 = current_q2.detach().mean().item()\n",
        "\n",
        "        # Critic loss is mean squared TD errors with priority weights.\n",
        "        #print((current_q1.detach().cpu().numpy()).shape) # (256,1)\n",
        "        #print((target_q).shape) # (256,)\n",
        "        #target_q = target_q.reshape((256,1))\n",
        "        #print((target_q).shape) # (256,1)\n",
        "        q1_loss = torch.mean((current_q1 - target_q)**2 * weights)\n",
        "        q2_loss = torch.mean((current_q2 - target_q)**2 * weights)\n",
        "        return q1_loss, q2_loss, errors, mean_q1, mean_q2\n",
        "\n",
        "    def calculate_actor_loss(self, batch, weights):\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "        # We re-sample actions to calculate expectations of Q.\n",
        "        sampled_action, entropy, _ = self.actor_model.sample(states)\n",
        "\n",
        "        # expectations of Q with clipped double Q technique\n",
        "        q1, q2 = self.critic_model(states, sampled_action)\n",
        "        q = torch.min(q1, q2)\n",
        "\n",
        "        # Policy objective is maximization of (Q + alpha * entropy) with priority weights.\n",
        "        actor_loss = torch.mean((- q - self.alpha * entropy) * weights)\n",
        "        return actor_loss, entropy\n",
        "\n",
        "    def train_episode(self):\n",
        "        self.episodes += 1\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "        done = False\n",
        "        state = self.env.reset()\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            action = self.act(state)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            #if(not done) : print(done) # done = True when the episode is finished\n",
        "            self.steps += 1\n",
        "            episode_steps += 1\n",
        "            episode_reward += reward\n",
        "\n",
        "            # ignore done if the agent reach time horizons\n",
        "            # (set done=True only when the agent fails)\n",
        "            if episode_steps >= 100:\n",
        "                masked_done = False\n",
        "            else:\n",
        "                masked_done = done\n",
        "\n",
        "            if self.per:\n",
        "                batch = to_batch(state, action, reward, next_state, masked_done, self.device)\n",
        "                with torch.no_grad():\n",
        "                    current_q1, current_q2 = self.critic_model(states, actions)\n",
        "\n",
        "                    next_actions, next_entropies, _ = self.actor_model.sample(next_states)\n",
        "                    next_q1, next_q2 = self.critic_target_model(next_states, next_actions)\n",
        "                    next_q = torch.min(next_q1, next_q2) + self.alpha * next_entropies\n",
        "                target_q = rewards + (1.0 - dones) * self.gamma_n * next_q\n",
        "                error = torch.abs(current_q1 - target_q).item()\n",
        "                # We need to give true done signal with addition to masked done\n",
        "                # signal to calculate multi-step rewards.\n",
        "                self.replay_buffer.insert(state, action, reward, next_state, masked_done, error, episode_done=done)\n",
        "\n",
        "            else:\n",
        "                # We need to give true done signal with addition to masked done\n",
        "                # signal to calculate multi-step rewards\n",
        "                self.replay_buffer.insert(state, action, reward, next_state, episode_done=done)\n",
        "\n",
        "            if self.is_update():\n",
        "                for _ in range(self.updates_per_step):\n",
        "                    self.learn()\n",
        "\n",
        "            if(self.steps % self.eval_interval==0):\n",
        "                self.evaluate()\n",
        "\n",
        "            state=next_state\n",
        "\n",
        "\n",
        "        # We log running mean of training rewards.\n",
        "        self.train_rewards.append(episode_reward)\n",
        "        print(self.train_rewards.get(), self.steps)\n",
        "\n",
        "        print(f'episode: {self.episodes:<4}  '\n",
        "              f'episode steps: {episode_steps:<4}  '\n",
        "              f'reward: {episode_reward:<5.1f}')\n",
        "\n",
        "    def evaluate(self):\n",
        "        print(\"NOW EVALUATE\")\n",
        "        env = gym.make(\"parking-v0\")\n",
        "        episodes = 10\n",
        "        returns = np.zeros((episodes,), dtype=np.float32)\n",
        "\n",
        "        env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0.\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.exploit(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "            returns[i] = episode_reward\n",
        "\n",
        "        mean_return = np.mean(returns)\n",
        "        \n",
        "        print('-' * 60)\n",
        "        print(f'Num steps: {self.steps:<5}  '\n",
        "              f'reward: {mean_return:<5.1f}')\n",
        "        print('-' * 60)\n",
        "\n",
        "        env.close()\n",
        "        show_video('./video')\n",
        "\n",
        "    def learn(self):\n",
        "        #print(\"NOW LEARN\")\n",
        "        self.learning_steps += 1\n",
        "        if self.learning_steps % self.target_update_interval == 0:\n",
        "            soft_update(self.critic_target_model, self.critic_model, self.tau)\n",
        "\n",
        "        if self.per:\n",
        "            batch, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
        "        else:\n",
        "            batch = self.replay_buffer.sample(self.batch_size)\n",
        "            weights = 1.\n",
        "\n",
        "        q1_loss, q2_loss, errors, mean_q1, mean_q2 = self.calculate_critic_loss(batch, weights)\n",
        "        actor_loss, entropies = self.calculate_actor_loss(batch, weights)\n",
        "\n",
        "        update_params(self.Q1_optimizer, self.critic_model.Q1, q1_loss, self.grad_clip)\n",
        "        update_params(self.Q2_optimizer, self.critic_model.Q2, q2_loss, self.grad_clip)\n",
        "        update_params(self.actor_optimizer, self.actor_model, actor_loss, self.grad_clip)\n",
        "\n",
        "        #loss.backward(create_graph=True)\n",
        "        q1_loss.backward(retain_graph=True)\n",
        "        q2_loss.backward(retain_graph=True)\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "\n",
        "        self.Q1_optimizer.step()\n",
        "        self.Q2_optimizer.step()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        if self.entropy_tuning:\n",
        "            entropy_loss = -torch.mean(self.log_alpha * (self.target_entropy - entropies).detach() * weights)\n",
        "            update_params(self.alpha_optim, None, entropy_loss)\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        if self.per:\n",
        "            # update priority weights\n",
        "            self.replay_buffer.update_priority(indices, errors.cpu().numpy())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2TmgMKWRvy4"
      },
      "source": [
        "def run():\n",
        "\n",
        "    # You can define configs in the external json or yaml file.\n",
        "    configs = {\n",
        "        'num_steps': 20000,\n",
        "        'batch_size': 256,\n",
        "        'lr': 0.0003,\n",
        "        'hidden_units': [256, 256],\n",
        "        'memory_size': 1e6,\n",
        "        'gamma': 0.99,\n",
        "        'tau': 0.005,\n",
        "        'entropy_tuning': True,\n",
        "        'ent_coef': 0.2,  # It's ignored when entropy_tuning=True.\n",
        "        'multi_step': 1,\n",
        "        'per': False,  # prioritized experience replay\n",
        "        'alpha': 0.6,  # It's ignored when per=False.\n",
        "        'beta': 0.4,  # It's ignored when per=False.\n",
        "        'beta_annealing': 0.0001,  # It's ignored when per=False.\n",
        "        'grad_clip': None,\n",
        "        'updates_per_step': 1,\n",
        "        'start_steps': 10000,\n",
        "        'log_interval': 10,\n",
        "        'target_update_interval': 1,\n",
        "        'eval_interval': 10000,\n",
        "        'seed': 0\n",
        "    }\n",
        "\n",
        "    env = gym.make(\"parking-v0\")\n",
        "\n",
        "    agent = Agent(env=env, **configs)\n",
        "    fuck = 0\n",
        "    agent.run(fuck)\n",
        "\n",
        "    ##-----------------------------------##\n",
        "\n",
        "    obs = agent.env.reset()\n",
        "    done =  False\n",
        "\n",
        "    # Evaluate the agent\n",
        "    rewards_total = []\n",
        "    episode_reward = 0\n",
        "    for _ in range(10000):\n",
        "      with torch.no_grad():\n",
        "        action = agent.exploit(obs)\n",
        "      obs, reward, done, info = agent.env.step(action)\n",
        "      episode_reward += reward\n",
        "      if done or info.get('is_success', False):\n",
        "        print(\"Reward:\", episode_reward, \"Success?\", info.get('is_success', False))\n",
        "        rewards_total.append(episode_reward)\n",
        "        episode_reward = 0.0\n",
        "        obs = agent.env.reset()\n",
        "    \n",
        "    print(rewards_total)\n",
        "    # plot the scores\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.plot(np.arange(1, len(rewards_total)+1), rewards_total)\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.show()    \n",
        "\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}