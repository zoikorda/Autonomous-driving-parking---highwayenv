{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Αντίγραφο DQN_parking_last_try.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoikorda/self-driving-cars/blob/main/DQN_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROfN4CgHKtHR"
      },
      "source": [
        "!pip install highway-env\n",
        "!pip install stable-baselines==2.10.0\n",
        "!pip install gym\n",
        "!pip install highway-env\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9X090Ckujpd"
      },
      "source": [
        "!pip install --user git+https://github.com/eleurent/rl-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTjOxWNJKSXp"
      },
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimizer\n",
        "import numpy as np\n",
        "import random\n",
        "#import rl-agents\n",
        "#import wandb\n",
        "from collections import deque\n",
        "#from rl_agents.configuration import Configurable\n",
        "from random import random\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "from random import sample\n",
        "\n",
        "#from rl_agents.agents.common.memory import Transition\n",
        "\n",
        "import gym\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCiCxol7rHAa"
      },
      "source": [
        "from collections import namedtuple\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSsseatgRdnA"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, observation_shape, num_actions):\n",
        "        super(Model, self).__init__()\n",
        "        assert len(observation_shape) == 1\n",
        "        self.observation_shape = observation_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(observation_shape[0], 256),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(256, num_actions)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optimizer.Adam(self.net.parameters(), lr=0.0001)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sYuyde22Mgb"
      },
      "source": [
        "class ReplayBuffer2:\n",
        "  \"\"\"\n",
        "  Stores and samples transitions\n",
        "  \"\"\"\n",
        "  def __init__(self, buffer_size=100000, transition_type=Transition):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.transition_type = transition_type\n",
        "    self.buffer = []\n",
        "    self.position = 0\n",
        "\n",
        "  def insert(self, *args):\n",
        "    \"\"\"\n",
        "    saves a transition\n",
        "    \"\"\"\n",
        "    if len(self.buffer) < self.buffer_size:\n",
        "      self.buffer.append(None)\n",
        "      self.position = len(self.buffer) - 1\n",
        "    elif len(self.buffer) > self.buffer_size:\n",
        "      self.buffer = self.buffer[:self.buffer_size]\n",
        "    self.buffer[self.position] = self.transition_type(*args)\n",
        "    self.position = (self.position + 1) % self.buffer_size          \n",
        "    \n",
        "  def sample(self, batch_size, num_steps=1, collapsed=True):\n",
        "    \"\"\"\n",
        "    Samples a batch of transitions\n",
        "    If num_steps>1 the batch will be composed of lists of successive transitions.\n",
        "    :param collapsed: whether successive transitions must be collapsed into one n-step transition.\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    \"\"\"\n",
        "    assert batch_size <= len(self.buffer)\n",
        "    if num_steps==1:\n",
        "      return sample(self.buffer, batch_size)\n",
        "    else:      \n",
        "      indexes = sample(range(len(self.buffer)), batch_size) # sample initial transition indexes\n",
        "      all_transitions = [self.buffer[i:i+num_steps] for i in indexes] # get the batch of n-consecutive-transitions starting from sampled indexes\n",
        "      return map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions # collapse transitions\n",
        "\n",
        "  def collapse_n_steps(self, transitions):\n",
        "    \"\"\"\n",
        "    Collapse n transitions <s,a,r,s',d> of a trajectory into one transition <s0, a0, Sum(r), sp, dp>.\n",
        "    We start from the initial state, perform the first action, and then the return estimate is formed by\n",
        "            accumulating the discounted rewards along the trajectory until a terminal state or the end of the\n",
        "            trajectory is reached.\n",
        "    :param transitions: A list of n successive transitions\n",
        "    \"\"\"\n",
        "    state, action, cumulated_reward, next_state, done = transitions[0]\n",
        "    discount = 1\n",
        "    for transition in transitions[1:]:\n",
        "      if done:\n",
        "        break\n",
        "      else:\n",
        "        _, _, reward, next_state, done = transition\n",
        "        discount *= gamma\n",
        "        cumulated_reward += discount*reward\n",
        "    return state, action, cumulated_reward, next_state, done"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMN85St3mQhU"
      },
      "source": [
        "def train1(model, batch, target, num_actions):\n",
        "\n",
        "  #print(batch)\n",
        "  print(\"THE FEATURES............\")\n",
        "\n",
        "  if not isinstance(batch.state, torch.Tensor):\n",
        "    current_states = torch.stack(([torch.Tensor(s[\"observation\"]) for s in batch.state]))\n",
        "    #print(current_states.shape) # torch.Size([256, 6])\n",
        "    actions = torch.stack(([torch.Tensor(s) for s in batch.action]))\n",
        "    #print(actions.shape) # torch.Size([256, 2])\n",
        "    rewards = torch.stack(([torch.Tensor([s]) for s in batch.reward]))\n",
        "    #print(rewards.shape) # torch.Size([256, 1])\n",
        "    next_states = torch.stack(([torch.Tensor(s[\"observation\"]) for s in batch.next_state]))\n",
        "    #print(next_states.shape) # torch.Size([256, 6])\n",
        "    masks = torch.stack(([torch.Tensor([0]) if s else torch.Tensor([1]) for s in batch.done]))\n",
        "    #print(masks.shape) # torch.Size([256, 1])\n",
        "    batch = Transition(current_states, actions, rewards, next_states, masks)\n",
        "  \n",
        "  loss_func = torch.nn.MSELoss()\n",
        "  model.optimizer.zero_grad()\n",
        "  q_values = model(current_states) # get current Q-values estimates\n",
        "  #print(q_values.shape) # torch.Size([256, 2])\n",
        "  q_values = torch.gather(q_values, dim=1, index=actions.long()) # retrieve the Q-values for the actions from the replay buffer\n",
        "  #print(q_values) \n",
        "  with torch.no_grad():\n",
        "    best_values = target(next_states) # compute the next Q-values using the target network\n",
        "    #print(best_values.shape) # torch.Size([256, 2])\n",
        "    best_values, _ = best_values.max(dim=1) # follow greedy policy: use the one with the highest value\n",
        "    best_values = best_values.reshape(-1,1) # avoid potential broadcast issue\n",
        "    \n",
        "    # plot the scores\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.plot(np.arange(1, len(rewards)+1), rewards)\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Episode steps #')\n",
        "    plt.show()\n",
        "\n",
        "    target_state_action_value = rewards + gamma * best_values*(1-masks) # 1-step TD target\n",
        "\n",
        "  loss = loss_func(q_values, target_state_action_value)\n",
        "  return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9aNln4fKmXS"
      },
      "source": [
        "gamma = 0.99 # the discount factor\n",
        "\n",
        "def update_target_model(model, target):\n",
        "  \"\"\"\n",
        "  update the target network every \"target_model_update\" epochs.\n",
        "  \"\"\"\n",
        "  target.load_state_dict(model.state_dict())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCvmazbaF-7E"
      },
      "source": [
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video(path):\n",
        "    html = []\n",
        "    for mp4 in Path(path).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjlXw4Xf9l00"
      },
      "source": [
        "def sample_minibatch(memory, batch_size):\n",
        "  print(\"------------MEMORY------------------\")\n",
        "  print(len(memory.buffer), batch_size)\n",
        "  if(len(memory.buffer) < batch_size):\n",
        "    return None\n",
        "  transitions = memory.sample(batch_size)\n",
        "  return Transition(*zip(*transitions))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdGxc_n_KrF1"
      },
      "source": [
        "import highway_env\n",
        "if __name__ == '__main__':\n",
        "    test = False\n",
        "    min_replay_buffer_size = 10000\n",
        "    sample_size = 2500\n",
        "\n",
        "    #epsilon_max = 1.0\n",
        "    epsilon_min = 0.01\n",
        "\n",
        "    epsilon_decay = 0.999995\n",
        "\n",
        "    env_steps_before_train = 100\n",
        "    target_model_update = 20 ###we use 20 for a faster training (more efficient with 150 episodes)\n",
        "    env = gym.make(\"parking-v0\")\n",
        "    last_observation = env.reset()\n",
        "    print(last_observation) \n",
        "    print(env.observation_space[\"observation\"].shape)\n",
        "\n",
        "    base_model = Model(env.observation_space[\"observation\"].shape, env.action_space.shape[0])\n",
        "\n",
        "    target_model = Model(env.observation_space[\"observation\"].shape, env.action_space.shape[0])\n",
        "    update_target_model(base_model, target_model)\n",
        "\n",
        "    replay_buffer = ReplayBuffer2()\n",
        "\n",
        "    steps_since_train = 0\n",
        "    epochs_since_target = 0\n",
        "    step_num = -1 * min_replay_buffer_size\n",
        "\n",
        "    episode_rewards = []\n",
        "    rolling_reward = 0\n",
        "\n",
        "    losses = []\n",
        "    since=0\n",
        "\n",
        "    tq = tqdm()\n",
        "    try:\n",
        "        while since!=4:\n",
        "            if test:\n",
        "              env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "              time.sleep(0.05)\n",
        "\n",
        "            tq.update(1)\n",
        "            eps = epsilon_decay**(step_num)\n",
        "\n",
        "            if test:\n",
        "              eps = 0\n",
        "\n",
        "            if random() < eps:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                  action = base_model(torch.Tensor(last_observation[\"observation\"]))\n",
        "                                                  \n",
        "            observation, reward, done, info = env.step(action)\n",
        "            rolling_reward += reward\n",
        "            reward = reward/100.0\n",
        "\n",
        "            replay_buffer.insert(last_observation, action, reward, observation, done)\n",
        "            last_observation = observation\n",
        "\n",
        "            if done:\n",
        "                print(\"IT IS DONE\")\n",
        "                episode_rewards.append(rolling_reward)\n",
        "                print(rolling_reward)\n",
        "                if test:\n",
        "                    print(rolling_reward)\n",
        "                rolling_reward = 0\n",
        "                observation = env.reset()\n",
        "\n",
        "            steps_since_train += 1\n",
        "            step_num += 1\n",
        "\n",
        "            if (not test) and len(replay_buffer.buffer) > min_replay_buffer_size and steps_since_train > env_steps_before_train:\n",
        "\n",
        "              batch = sample_minibatch(replay_buffer, sample_size) \n",
        "              loss1 = train1(base_model, batch, target_model, env.action_space.shape[0])\n",
        "              losses.append(loss1)\n",
        "              print(\"LOSS\", loss1)\n",
        "              episode_rewards = []\n",
        "              epochs_since_target += 1\n",
        "              print(epochs_since_target)\n",
        "              if epochs_since_target > target_model_update:\n",
        "                  print(\"Updating target model----------------------\")\n",
        "                  env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "                  update_target_model(base_model, target_model)\n",
        "                  epochs_since_target = 0\n",
        "                  since += 1\n",
        "                  show_video('./video')\n",
        "              steps_since_train = 0\n",
        "            env.close()\n",
        "            show_video('./video')  \n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sxhf1Q6NlL"
      },
      "source": [
        "obs = env.reset()\n",
        "done =  False\n",
        "\n",
        "# Evaluate the agent\n",
        "rewards_total = []\n",
        "episode_reward = 0\n",
        "for _ in range(10000):\n",
        "  with torch.no_grad():\n",
        "    action = base_model(torch.Tensor(obs[\"observation\"]))\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  episode_reward += reward\n",
        "  if done or info.get('is_success', False):\n",
        "    print(\"Reward:\", episode_reward, \"Success?\", info.get('is_success', False))\n",
        "    rewards_total.append(episode_reward)\n",
        "    episode_reward = 0.0\n",
        "    obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq63cXwWQhf3"
      },
      "source": [
        "print(rewards_total)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(rewards_total)+1), rewards_total)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaBH-LXsAEEc"
      },
      "source": [
        "# Plot losses\n",
        "plt.plot(losses)\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"episodes\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}